{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Food101_classifier_EfficientnetB1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "j0iFXC6Bdcz8",
        "colab_type": "code",
        "outputId": "180f28f6-5e98-4f74-8a0e-a66125c2b7b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!nvidia-smi -L"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "GPU 0: Tesla K80 (UUID: GPU-414396f5-b177-f908-58bb-147eacbefddb)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4F5bUoCSi0gu",
        "colab_type": "code",
        "outputId": "f1469500-fb55-47fe-c269-14946c595369",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 201
        }
      },
      "source": [
        "!pip show keras-efficientnets"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Name: keras-efficientnets\n",
            "Version: 0.1.6.1\n",
            "Summary: Keras implementation of EfficientNets of any configuration.\n",
            "Home-page: https://github.com/titu1994/keras-efficientnets\n",
            "Author: Somshubra Majumdar\n",
            "Author-email: titu1994@gmail.com\n",
            "License: MIT\n",
            "Location: /usr/local/lib/python3.6/dist-packages\n",
            "Requires: keras, scipy, scikit-learn\n",
            "Required-by: \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jBdjH6VhWduO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install keras-efficientnets"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K5YMdcYi5cF8",
        "colab_type": "code",
        "outputId": "4c8c55c9-1b0a-43f6-84ab-9ef5a01a3b07",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        }
      },
      "source": [
        "# Code to read csv file into colaboratory:\n",
        "!pip install -U -q PyDrive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "\n",
        "# 1. Authenticate and create the PyDrive client.\n",
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "\n",
        "drive = GoogleDrive(gauth)\n",
        "\n",
        " "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[?25l\r\u001b[K     |▎                               | 10kB 18.9MB/s eta 0:00:01\r\u001b[K     |▋                               | 20kB 1.8MB/s eta 0:00:01\r\u001b[K     |█                               | 30kB 2.6MB/s eta 0:00:01\r\u001b[K     |█▎                              | 40kB 1.7MB/s eta 0:00:01\r\u001b[K     |█▋                              | 51kB 2.1MB/s eta 0:00:01\r\u001b[K     |██                              | 61kB 2.5MB/s eta 0:00:01\r\u001b[K     |██▎                             | 71kB 2.9MB/s eta 0:00:01\r\u001b[K     |██▋                             | 81kB 3.3MB/s eta 0:00:01\r\u001b[K     |███                             | 92kB 3.7MB/s eta 0:00:01\r\u001b[K     |███▎                            | 102kB 2.8MB/s eta 0:00:01\r\u001b[K     |███▋                            | 112kB 2.8MB/s eta 0:00:01\r\u001b[K     |████                            | 122kB 2.8MB/s eta 0:00:01\r\u001b[K     |████▎                           | 133kB 2.8MB/s eta 0:00:01\r\u001b[K     |████▋                           | 143kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████                           | 153kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████▎                          | 163kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████▋                          | 174kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████                          | 184kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 194kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████▋                         | 204kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████                         | 215kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████▎                        | 225kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████▋                        | 235kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████                        | 245kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████▎                       | 256kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████▋                       | 266kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████                       | 276kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████▎                      | 286kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████▋                      | 296kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████                      | 307kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████▎                     | 317kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████▋                     | 327kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████                     | 337kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████▎                    | 348kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████▋                    | 358kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████                    | 368kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████▎                   | 378kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 389kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████                   | 399kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████▎                  | 409kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████▋                  | 419kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████                  | 430kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████▎                 | 440kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 450kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████                 | 460kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████▎                | 471kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████▋                | 481kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████                | 491kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████▎               | 501kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████▋               | 512kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████               | 522kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▎              | 532kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████▋              | 542kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████              | 552kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▎             | 563kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████▋             | 573kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████             | 583kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▎            | 593kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████▋            | 604kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████            | 614kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▎           | 624kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████▋           | 634kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 645kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▎          | 655kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████▋          | 665kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████          | 675kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▎         | 686kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████▋         | 696kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 706kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▎        | 716kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████▋        | 727kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████        | 737kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▎       | 747kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▋       | 757kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████▉       | 768kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▏      | 778kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▌      | 788kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▉      | 798kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▏     | 808kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▌     | 819kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████▉     | 829kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 839kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▌    | 849kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▉    | 860kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▏   | 870kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▌   | 880kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████▉   | 890kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 901kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 911kB 2.8MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▉  | 921kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▏ | 931kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▌ | 942kB 2.8MB/s eta 0:00:01\r\u001b[K     |██████████████████████████████▉ | 952kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▏| 962kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▌| 972kB 2.8MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▉| 983kB 2.8MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 993kB 2.8MB/s \n",
            "\u001b[?25h  Building wheel for PyDrive (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "fpbsmDszW2Yl",
        "colab": {}
      },
      "source": [
        "#downloading the dataset\n",
        "!gsutil cp gs://tibot-adnan/Food_dataset/food_dataset.zip food_dataset.zip"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ISCj8onSW2Ys",
        "colab": {}
      },
      "source": [
        "#unzipping the dataset\n",
        "!unzip food_dataset.zip\n",
        "!ls"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20TjhjcxPILN",
        "colab_type": "code",
        "outputId": "84a4c1ac-b4e1-4363-ddea-af361e801db6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "#importing necessary packages\n",
        "import numpy as np\n",
        "import keras\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import itertools\n",
        "from math import ceil\n",
        "from keras import applications\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dropout, Flatten, Dense\n",
        "from keras.layers import Activation\n",
        "from keras.optimizers import SGD,adam,nadam\n",
        "from keras.metrics import categorical_crossentropy\n",
        "from keras import optimizers\n",
        "from sklearn.metrics import confusion_matrix,roc_auc_score\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.callbacks import ModelCheckpoint\n",
        "%matplotlib inline\n",
        "\n",
        "img_width, img_height = 224,224\n",
        "\n",
        "train_data_dir = 'food_dataset/train'\n",
        "test_data_dir=   'food_dataset/test'\n",
        "nb_train_sample = 80800\n",
        "nb_valid_sample = 10100\n",
        "nb_test_sample=10100\n",
        "#nb_test_sample=80\n",
        "epochs = 50\n",
        "batch_size = 60\n",
        "multiplier=1\n",
        "steps_train = ceil(multiplier*(nb_train_sample/batch_size))\n",
        "steps_valid= ceil(multiplier*(nb_valid_sample/batch_size))\n",
        "steps_test= ceil(nb_test_sample/1)\n",
        "print(\"done\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "done\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qgCsT0txPOud",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "'''Creating the training and testing image generator for feeding the images in the model\n",
        "'''\n",
        "train_datagen = ImageDataGenerator(\n",
        "rescale=1. / 255,\n",
        "channel_shift_range=30,\n",
        "zoom_range=0.2,\n",
        "width_shift_range=0.2,  # randomly shift images horizontally (fraction of total width)\n",
        "height_shift_range=0.2,  # randomly shift images vertically (fraction of total height)\n",
        "horizontal_flip=True,# randomly flip images\n",
        "\n",
        "fill_mode='reflect',\n",
        "validation_split= 0.1111111111111111)#for validation\n",
        "\n",
        "train_generator = train_datagen.flow_from_directory(\n",
        "train_data_dir,\n",
        "target_size=(img_width, img_height),\n",
        "batch_size=batch_size,\n",
        "class_mode='sparse',shuffle = True,\n",
        "subset='training',seed=42) # set as training data\n",
        "\n",
        "validation_generator = train_datagen.flow_from_directory(\n",
        "train_data_dir,#same train directory\n",
        "target_size=(img_width, img_height),\n",
        "batch_size=batch_size,\n",
        "class_mode='sparse',shuffle = True,\n",
        "subset='validation',seed=42) # set as validation data\n",
        "\n",
        "# data generator for test set\n",
        "test_datagen = ImageDataGenerator(rescale=1. / 255,)\n",
        "\n",
        "# generator for reading test data from folder\n",
        "test_generator = test_datagen.flow_from_directory(\n",
        "    test_data_dir,\n",
        "    target_size = (img_width, img_height),\n",
        "    \n",
        "    color_mode = 'rgb',\n",
        "    batch_size = 1,\n",
        "    class_mode = 'sparse',\n",
        "    shuffle = False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bdzpI-SnW_5t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#defining the model\n",
        "from keras import backend as K\n",
        "from keras_efficientnets import EfficientNetB1\n",
        "from keras.models import Model\n",
        "from keras.layers import Dense, GlobalAveragePooling2D\n",
        " \n",
        "base_model = EfficientNetB1( include_top=False, weights='imagenet') #defining the model according to the package description\n",
        "x = base_model.output\n",
        "x = GlobalAveragePooling2D()(x)\n",
        "predictions = Dense(101, activation='softmax')(x)\n",
        "\n",
        "# this is the model we will train\n",
        "model = Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "# first: train only the top layers (which were randomly initialized)\n",
        "# i.e. freeze all  InceptionV3 layers\n",
        "#for layer in model.layers[:-1]:\n",
        "#    layer.trainable = False\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OGr9y-N2Zf_x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#getting the model schema\n",
        "a=model.to_json()\n",
        "print(a)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nF6tVvuKNVfW",
        "colab_type": "text"
      },
      "source": [
        "**These following 3 cells are for determining the learning rate at the start of the training if needed. Run it if you wish to find the learning rate.**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "azanjcgSBuzb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from keras.callbacks import Callback\n",
        "import keras.backend as K\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "class LRFinder(Callback):\n",
        "    def __init__(self, min_lr, max_lr, mom=0.9, stop_multiplier=None, \n",
        "                 reload_weights=True, batches_lr_update=5):\n",
        "        self.min_lr = min_lr\n",
        "        self.max_lr = max_lr\n",
        "        self.mom = mom\n",
        "        self.reload_weights = reload_weights\n",
        "        self.batches_lr_update = batches_lr_update\n",
        "        if stop_multiplier is None:\n",
        "            self.stop_multiplier = -20*self.mom/3 + 10 # 4 if mom=0.9\n",
        "                                                       # 10 if mom=0\n",
        "        else:\n",
        "            self.stop_multiplier = stop_multiplier\n",
        "        \n",
        "    def on_train_begin(self, logs={}):\n",
        "        p = self.params\n",
        "        try:\n",
        "            n_iterations = p['epochs']*p['samples']//p['batch_size']\n",
        "        except:\n",
        "            n_iterations = p['steps']*p['epochs']\n",
        "            \n",
        "        self.learning_rates = np.geomspace(self.min_lr, self.max_lr, \\\n",
        "                                           num=n_iterations//self.batches_lr_update+1)\n",
        "        self.losses=[]\n",
        "        self.iteration=0\n",
        "        self.best_loss=0\n",
        "        if self.reload_weights:\n",
        "            self.model.save_weights('tmp.hdf5')\n",
        "        \n",
        "    \n",
        "    def on_batch_end(self, batch, logs={}):\n",
        "        loss = logs.get('loss')\n",
        "        \n",
        "        if self.iteration!=0: # Make loss smoother using momentum\n",
        "            loss = self.losses[-1]*self.mom+loss*(1-self.mom)\n",
        "        \n",
        "        if self.iteration==0 or loss < self.best_loss: \n",
        "                self.best_loss = loss\n",
        "                \n",
        "        if self.iteration%self.batches_lr_update==0: # Evaluate each lr over 5 epochs\n",
        "            \n",
        "            if self.reload_weights:\n",
        "                self.model.load_weights('tmp.hdf5')\n",
        "          \n",
        "            lr = self.learning_rates[self.iteration//self.batches_lr_update]            \n",
        "            K.set_value(self.model.optimizer.lr, lr)\n",
        "\n",
        "            self.losses.append(loss)            \n",
        "\n",
        "        if loss > self.best_loss*self.stop_multiplier: # Stop criteria\n",
        "            self.model.stop_training = True\n",
        "                \n",
        "        self.iteration += 1\n",
        "    \n",
        "    def on_train_end(self, logs=None):\n",
        "        if self.reload_weights:\n",
        "                self.model.load_weights('tmp.hdf5')\n",
        "                \n",
        "        plt.figure(figsize=(12, 6))\n",
        "        plt.plot(self.learning_rates[:len(self.losses)], self.losses)\n",
        "        plt.xlabel(\"Learning Rate\")\n",
        "        plt.ylabel(\"Loss\")\n",
        "        plt.xscale('log')\n",
        "        plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gTM0u1Tp-2cl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(optimizer='adam', \n",
        "              loss='sparse_categorical_crossentropy', \n",
        "              metrics=['accuracy'])\n",
        "lr_finder = LRFinder(min_lr=0.00001, max_lr=1)\n",
        "\n",
        "model.fit_generator(train_generator, steps_per_epoch=steps_train,callbacks=[lr_finder])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "jejkWrMVQme5",
        "colab": {}
      },
      "source": [
        "print(lr_finder.best_loss)\n",
        "print(min(lr_finder.losses))\n",
        "print(np.argmin(lr_finder.losses))\n",
        "print(lr_finder.learning_rates[np.argmin(lr_finder.losses)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-ilO91XN_ch",
        "colab_type": "text"
      },
      "source": [
        "RUN from Here"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQS2uZ0sQmJS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Custom callback for getting learning rate each epoch\n",
        "from keras.callbacks import Callback,History\n",
        "from keras import backend as K\n",
        "class showLR( Callback ) :\n",
        "    def on_epoch_begin(self, epoch, logs=None):\n",
        "        lr = float(K.get_value(self.model.optimizer.lr))\n",
        "        print (\" epoch={:02d}, lr={:.8f}\".format( epoch, lr ))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UsnorGWnQCnW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Defining optimizer and metrics for the model\n",
        "import functools\n",
        "top3_acc = functools.partial(keras.metrics.sparse_top_k_categorical_accuracy, k=3)\n",
        "top3_acc.__name__ = 'top3_acc'\n",
        "top5_acc = functools.partial(keras.metrics.sparse_top_k_categorical_accuracy, k=5)\n",
        "top5_acc.__name__ = 'top5_acc'\n",
        "\n",
        "showlr=showLR()\n",
        "history=History()\n",
        "checkpoint = ModelCheckpoint('FCEfnetB2-{epoch:03d}-04_08.h5', verbose=1, monitor='val_loss',save_best_only=False, mode='auto')\n",
        "Adam = adam(lr=.0005, beta_1=0.9, beta_2=0.999999, decay=1e-6)\n",
        "model.compile(loss='sparse_categorical_crossentropy',optimizer=Adam,metrics=['accuracy',top3_acc,top5_acc])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nQfs0jw5QE_2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.fit_generator(train_generator,steps_per_epoch=steps_train,validation_data=validation_generator,\n",
        "                    validation_steps=steps_valid,epochs=5,callbacks=[checkpoint,history,showlr], verbose=1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "SDYhAlxClY6K",
        "colab": {}
      },
      "source": [
        "#cell for testing the models\n",
        "acc=[]\n",
        "top3acc=[]\n",
        "top5acc=[]\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report\n",
        "\n",
        "#for later use \n",
        "for i in range(1,6):\n",
        "  file_number='{:03d}'.format(i)\n",
        "  \n",
        "  file_name='FCEfnetB2-'+file_number+'-04_08.h5'\n",
        "  model.load_weights(file_name)\n",
        "    test_generator.reset()\n",
        "  predictions = model.predict_generator(test_generator,steps = steps_test,verbose=1)\n",
        "  accuracy=accuracy_score(test_generator.classes,predictions.argmax(axis=1))*100\n",
        "  print('top1 Accuracy for weight file {} = {:3f}%'.format(file_name,accuracy))\n",
        "  top5 = 0.0  \n",
        "  top3 = 0.0\n",
        "  for idx, l in enumerate(test_generator.classes):\n",
        "    prediction = predictions[idx]\n",
        "    top3_values = (-prediction).argsort()[:3]\n",
        "    if np.isin(np.array([l]), top3_values):\n",
        "        top3 += 1.0\n",
        "    top_values = (-prediction).argsort()[:5]\n",
        "    if np.isin(np.array([l]), top_values):\n",
        "        top5 += 1.0\n",
        "        \n",
        "    \n",
        "  top3acc.append(top3/len(test_generator.classes)*100)\n",
        "  top5acc.append(top5/len(test_generator.classes)*100)\n",
        "  print(\"top3 acc= {:.03f}%\".format(top3acc[i-1]))\n",
        "  print(\"top5 acc= {:.03f}%\".format(top5acc[i-1]))\n",
        "  cm=confusion_matrix(test_generator.classes,predictions.argmax(axis=1))\n",
        "  print(cm)\n",
        "  print(classification_report(test_generator.classes, predictions.argmax(axis=1)))\n",
        "  sns.heatmap(cm)\n",
        "  plt.show()\n",
        "  #roc_auc_score(test_generator.classes, predictions.argmax(axis=1))\n",
        "  acc.append(accuracy)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyYNxrfLizTk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(acc)\n",
        "print(top3acc)\n",
        "print(top5acc)\n",
        "print(np.max(acc))\n",
        "print(np.max(top3acc))\n",
        "print(np.max(top5acc))\n",
        "print(np.argmax(acc))\n",
        "print(np.argmax(top3acc))\n",
        "print(np.argmax(top5acc))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}